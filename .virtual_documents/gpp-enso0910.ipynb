import sys
print(sys.executable)



"""READ DATA
read NetCDF print lat/lon/time 
"""

from pathlib import Path
import xarray as xr

DATA_DIR = Path(r"D:\cangku\data")

# dir
FILES = [
    "gpp_CanESM5_abrupt-4xCO2_185001-194912_1x1.nc",
    "gpp_Lmon_CanESM5_abrupt-4xCO2_r1i1p2f1_gn_185001-200012.nc",
    "gpp_Lmon_CanESM5_G1_r1i1p2f1_gn_185001-194912.nc",
    "gpp_Lmon_CanESM5_piControl_r1i1p2f1_gn_185001-194912.nc",
    "tos_Omon_CanESM5_abrupt-4xCO2_r1i1p2f1_gn_185001-200012.nc",
    "tos_Omon_CanESM5_G1_r1i1p2f1_gn_185001-194912.nc",
    "tos_Omon_CanESM5_piControl_r1i1p2f1_gn_185001-194912.nc",
]
# =================================

def _format_time_value(v) -> str:
    """ cftime and numpy.datetime64"""
    try:
        return str(v)
    except Exception:
        try:
            # 
            return v.isoformat()
        except Exception:
            return repr(v)

def _first_last_as_str(da) -> str:
    """time 'first → last'。"""
    if da.size == 0:
        return "(empty)"
    first = _format_time_value(da.values[0])
    last = _format_time_value(da.values[-1])
    return f"{first} → {last}"

def _find_coord_name(ds: xr.Dataset, candidates) -> str | None:
    """
     ds.coords ; attrs 
    return name or None。
    """
    # 1) find var name
    lower_map = {name.lower(): name for name in ds.coords}
    for cand in candidates:
        if cand in lower_map:
            return lower_map[cand]

    # 2) 
    for var in list(ds.coords) + list(ds.data_vars):
        v = ds[var]
        std = (v.attrs.get("standard_name", "") or "").lower()
        longn = (v.attrs.get("long_name", "") or "").lower()
        units = (v.attrs.get("units", "") or "").lower()
        # 
        if any(k in std for k in ["latitude"]) or any(k in longn for k in ["latitude"]) or units in ("degrees_north", "degree_north"):
            if "lat" in candidates[0]:  
                return var
        if any(k in std for k in ["longitude"]) or any(k in longn for k in ["longitude"]) or units in ("degrees_east", "degree_east"):
            if "lon" in candidates[0]:  
                return var
    return None

def get_basic_info(path: Path) -> dict:
    """
    open NetCDF :
      - lat / lon 
      - time 
    API：decode_times=CFDatetimeCoder(use_cftime=True) 
    """
    info = {}
    time_coder = xr.coders.CFDatetimeCoder(use_cftime=True)

    with xr.open_dataset(path, decode_times=time_coder) as ds:
        
        coord_names_lower = [c.lower() for c in ds.coords]
        # find lat / lon / time
        lat_candidates = ["lat", "latitude", "nav_lat", "y"]
        lon_candidates = ["lon", "longitude", "nav_lon", "x"]
        time_candidates = ["time"]

        lat_name = _find_coord_name(ds, lat_candidates)
        lon_name = _find_coord_name(ds, lon_candidates)

        # time 
        time_name = None
        for c in ds.coords:
            if c.lower() in time_candidates or "time" in c.lower():
                time_name = c
                break

        # info
        if lat_name is not None and lat_name in ds:
            info["lat_name"] = lat_name
            info["lat_shape"] = tuple(ds[lat_name].shape)
        else:
            info["lat_name"] = None
            info["lat_shape"] = None

        if lon_name is not None and lon_name in ds:
            info["lon_name"] = lon_name
            info["lon_shape"] = tuple(ds[lon_name].shape)
        else:
            info["lon_name"] = None
            info["lon_shape"] = None

        if time_name is not None and time_name in ds:
            t = ds[time_name]
            info["time_name"] = time_name
            info["time_len"] = int(t.size)
            info["time_range"] = _first_last_as_str(t)
        else:
            info["time_name"] = None
            info["time_len"] = 0
            info["time_range"] = "(not found)"

    return info

def main():
    print("=== basic information check ===")
    base = Path(DATA_DIR)
    for fname in FILES:
        path = base / fname
        if not path.exists():
            print(f"[lose] {fname}")
            continue

        try:
            info = get_basic_info(path)
            print(f"\nfile: {fname}")
            print(f"  lat:  {info.get('lat_name')}    shape={info.get('lat_shape')}")
            print(f"  lon:  {info.get('lon_name')}    shape={info.get('lon_shape')}")
            print(f"  time: {info.get('time_name')}   len={info.get('time_len')}")
            print(f"  time_range: {info.get('time_range')}")
        except Exception as e:
            print(f"[error] read {fname} fail：{e}")

    print("\n=== finish ===")

if __name__ == "__main__":
    main()



# -*- coding: utf-8 -*-
"""REGRID (clean version)
 6  NetCDF ——> 1°×1° ,print lat/lon/time 
- save gpp/tos + time + lat/lon
- large file (tos abrupt, time>1500)，sizes
"""

from pathlib import Path
import numpy as np
import xarray as xr

# ==========
DATA_DIR = Path(r"D:\cangku\data")
FILES = [
    "gpp_Lmon_CanESM5_abrupt-4xCO2_r1i1p2f1_gn_185001-200012.nc",
    "gpp_Lmon_CanESM5_G1_r1i1p2f1_gn_185001-194912.nc",
    "gpp_Lmon_CanESM5_piControl_r1i1p2f1_gn_185001-194912.nc",
    "tos_Omon_CanESM5_abrupt-4xCO2_r1i1p2f1_gn_185001-200012.nc",
    "tos_Omon_CanESM5_G1_r1i1p2f1_gn_185001-194912.nc",
    "tos_Omon_CanESM5_piControl_r1i1p2f1_gn_185001-194912.nc",
]
# =======================

# grid
target_lons = np.arange(0, 360, 1)
target_lats = np.arange(-90, 90, 1)

def fix_lon(lon):
    """lon 0–360"""
    lon = np.where(lon < 0, lon + 360, lon)
    lon = np.where(lon >= 360, lon - 360, lon)
    return lon

def preprocess_latlon(ds, lat_name, lon_name):
    """lat lon"""
    lat = ds[lat_name]
    lon = ds[lon_name]

    if lat.ndim == 2 and lon.ndim == 2:
        # === TOS information===
        lat_1d = lat[:, 0].values
        lon_1d = lon[0, :].values
        lon_1d = fix_lon(lon_1d)

        #  2D lat/lon
        ds = ds.drop_vars([lat_name, lon_name], errors="ignore")

        #  (time,j,i) -> (time,lat,lon)
        for v in list(ds.data_vars):
            if {"j", "i"}.issubset(ds[v].dims):
                ds[v] = ds[v].rename({"j": "lat", "i": "lon"})

        # add gird
        ds = ds.assign_coords(lat=("lat", lat_1d),
                              lon=("lon", lon_1d))
    else:
        # GPP 
        ds = ds.rename({lat_name: "lat", lon_name: "lon"})
        ds = ds.assign_coords(lon=fix_lon(ds["lon"].values))

    # lon sort
    ds = ds.sortby("lon")
    return ds

def get_latlon_names(ds):
    lat_name, lon_name = None, None
    for cand in ["lat", "latitude", "nav_lat", "y"]:
        if cand in ds.variables:
            lat_name = cand
            break
    for cand in ["lon", "longitude", "nav_lon", "x"]:
        if cand in ds.variables:
            lon_name = cand
            break
    if lat_name is None or lon_name is None:
        raise ValueError("fail read lon lat name")
    return lat_name, lon_name

def print_basic_info(ds, fname):
    print(f"\nfile: {fname}")
    for v in ds.data_vars:
        print(f"  vars: {v}, shape={ds[v].shape}, dims={ds[v].dims}, dtype={ds[v].dtype}")
    if "lat" in ds.coords:
        print(f"  lat: shape={ds['lat'].shape}, range={ds['lat'].values[0]} → {ds['lat'].values[-1]}")
    if "lon" in ds.coords:
        print(f"  lon: shape={ds['lon'].shape}, range={ds['lon'].values[0]} → {ds['lon'].values[-1]}")
    if "time" in ds.coords:
        t = ds["time"]
        print(f"  time: len={t.size}, range={t.values[0]} → {t.values[-1]}")

def regrid_one(path: Path):
    with xr.open_dataset(path) as ds:
        # lat_name, lon_name
        lat_name, lon_name = get_latlon_names(ds)

        # ⭐ save (gpp/tos) lat_name, lon_name ⭐
        if "gpp" in ds.data_vars:
            ds = ds[["gpp", "time", lat_name, lon_name]]
        elif "tos" in ds.data_vars:
            ds = ds[["tos", "time", lat_name, lon_name]]
        else:
            raise ValueError("未找到 gpp 或 tos 变量")

        # lat lon
        ds = preprocess_latlon(ds, lat_name, lon_name)

        # regrid
        if "tos" in path.name and ds.sizes["time"] > 1500:
            print(f"  [size regrid] {path.name}")
            chunks = []
            time_chunks = [
                slice("1850-01-01", "1900-12-31"),
                slice("1901-01-01", "1950-12-31"),
                slice("1951-01-01", "2000-12-31"),
            ]
            for tsel in time_chunks:
                ds_sel = ds.sel(time=tsel)
                ds_out_chunk = ds_sel.interp(
                    lon=target_lons,
                    lat=target_lats,
                    method="linear"
                )
                chunks.append(ds_out_chunk)
            ds_out = xr.concat(chunks, dim="time")
            ds_out = ds_out.sortby("time")
        else:
            ds_out = ds.interp(
                lon=target_lons,
                lat=target_lats,
                method="linear"
            )

    out_path = path.with_name(path.stem + "_1x1.nc")
    ds_out.to_netcdf(out_path)
    print(f"[finish] {path.name} -> {out_path.name}")
    print_basic_info(ds_out, out_path.name)

def main():
    for fname in FILES:
        path = DATA_DIR / fname
        if path.exists():
            try:
                regrid_one(path)
            except Exception as e:
                print(f"[error] {fname}: {e}")
        else:
            print(f"[error] {fname}")

if __name__ == "__main__":
    main()



# -*- coding: utf-8 -*-
"""DETREND (1890–1949)

1.  1890-1949 (720 months)
2. (gpp  tos)
3.  XXXX = 1..720
4. detrend with time 
5. save (_1890-1949_detrend.nc)
"""

import xarray as xr
import numpy as np
from pathlib import Path

DATA_DIR = Path(r"D:\cangku\data")

FILES = [
    "gpp_Lmon_CanESM5_abrupt-4xCO2_r1i1p2f1_gn_185001-200012_1x1.nc",
    "gpp_Lmon_CanESM5_G1_r1i1p2f1_gn_185001-194912_1x1.nc",
    "gpp_Lmon_CanESM5_piControl_r1i1p2f1_gn_185001-194912_1x1.nc",
    "tos_Omon_CanESM5_abrupt-4xCO2_r1i1p2f1_gn_185001-200012_1x1.nc",
    "tos_Omon_CanESM5_G1_r1i1p2f1_gn_185001-194912_1x1.nc",
    "tos_Omon_CanESM5_piControl_r1i1p2f1_gn_185001-194912_1x1.nc",
]

# 1D timeseries 1..720 (1890–1949 = 60 years × 12 months)
XXXX = np.arange(1, 721, dtype="float64")

def detrend_3d(data):
    """
    input: data (time, lat, lon) -> numpy array
    output: detrend data (time, lat, lon)
    """
    t, nlat, nlon = data.shape
    out = np.full_like(data, np.nan, dtype="float64")

    for i in range(nlat):
        for j in range(nlon):
            y = data[:, i, j]
            if np.all(np.isnan(y)):
                continue
            mask = np.isfinite(y)
            if mask.sum() > 1:
                p = np.polyfit(XXXX[mask], y[mask], 1)
                trend = np.polyval(p, XXXX)
                out[:, i, j] = y - trend
    return out

def process_one(path: Path):
    with xr.open_dataset(path) as ds:
        # main vars gpp/tos
        main_var = None
        for cand in ["gpp", "tos"]:
            if cand in ds.data_vars:
                main_var = cand
                break
        if main_var is None:
            print(f"[jump] {path.name}: fail find gpp or tos")
            return

        # sel time 1890-1949
        da = ds[main_var].sel(time=slice("1890-01-01", "1949-12-31")).astype("float64")

        #  numpy (time, lat, lon)
        data = da.values
        if da.dims != ("time", "lat", "lon"):
            raise ValueError(f"{main_var} error: {da.dims}")

        # detrend
        detrended = detrend_3d(data)

        # return DataArray
        da_dt = xr.DataArray(
            detrended,
            coords={"time": da["time"], "lat": da["lat"], "lon": da["lon"]},
            dims=("time", "lat", "lon"),
            name=main_var,
            attrs=da.attrs,
        )

        # save
        out_path = path.with_name(path.stem + "_1890-1949_detrend.nc")
        da_dt.to_netcdf(out_path)
        print(f"[finish] {path.name} -> {out_path.name}")

def main():
    for fname in FILES:
        path = DATA_DIR / fname
        if path.exists():
            try:
                process_one(path)
            except Exception as e:
                print(f"[error] {fname}: {e}")
        else:
            print(f"[error] {fname}")

if __name__ == "__main__":
    main()



# -*- coding: utf-8 -*-
import xarray as xr
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
from pathlib import Path

DATA_DIR = Path(r"D:\cangku\data")

FILES = {
    "abrupt-4xCO2": "tos_Omon_CanESM5_abrupt-4xCO2_r1i1p2f1_gn_185001-200012_1x1_1890-1949_detrend.nc",
    "G1": "tos_Omon_CanESM5_G1_r1i1p2f1_gn_185001-194912_1x1_1890-1949_detrend.nc",
    "piControl": "tos_Omon_CanESM5_piControl_r1i1p2f1_gn_185001-194912_1x1_1890-1949_detrend.nc",
}

lat_bounds = (-5, 5)
lon_bounds = (190, 240)

def calc_djf_index(ds):
    tos = ds["tos"]
    tos_sel = tos.sel(lat=slice(*lat_bounds), lon=slice(*lon_bounds))
    tos_mean = tos_sel.mean(dim=("lat", "lon"))

    # DJF season mean
    djf = tos_mean.rolling(time=3, center=True).mean()
    djf = djf.sel(time=djf["time.month"] == 1)
    djf = djf.sel(time=slice("1890-01-01", "1949-12-31"))

    values = djf.values
    values = (values - np.nanmean(values)) / np.nanstd(values)
    return values, djf["time"].dt.year.values

ENSO_index = np.zeros((3, 60))
years = None
for k, fname in enumerate(FILES.values()):
    ds = xr.open_dataset(DATA_DIR / fname)
    values, yrs = calc_djf_index(ds)
    ENSO_index[k, :] = values
    if years is None:
        years = yrs
    ds.close()

print("ENSO_index shape:", ENSO_index.shape)

# ===== draw picture =====
fig, axes = plt.subplots(3, 1, figsize=(10, 9), sharex=True)

for i, (exp, fname) in enumerate(FILES.items()):
    ax = axes[i]
    vals = ENSO_index[i, :]

    # background color：El Niño / La Niña ( ±0.5)
    ax.fill_between(years, -5, 5, where=vals > 0.5, color="orange", alpha=0.3)
    ax.fill_between(years, -5, 5, where=vals < -0.5, color="purple", alpha=0.3)

    ax.plot(years, vals, color="black", linewidth=1.2)
    ax.scatter(years, vals, color="black", s=15, zorder=3)

    # y=0, ±0.5 
    ax.axhline(0, color="gray", linewidth=0.8, linestyle="--")
    ax.axhline(0.5, color="gray", linewidth=0.8, linestyle="--")
    ax.axhline(-0.5, color="gray", linewidth=0.8, linestyle="--")

    ax.set_ylim(-3, 3)
    ax.set_ylabel("Standardized DJF Niño3.4")
    ax.set_title(exp)

    # legend
    legend_handles = [
        Patch(facecolor="orange", alpha=0.3, label="El Niño (≥ +0.5σ)"),
        Patch(facecolor="purple", alpha=0.3, label="La Niña (≤ -0.5σ)")
    ]
    ax.legend(handles=legend_handles, loc="upper right", frameon=False)

axes[-1].set_xlabel("Year")
axes[-1].set_xlim(1890, 1950)
axes[-1].set_xticks(np.arange(1890, 1951, 5))

plt.tight_layout()

# save
fig.suptitle("Standardized DJF Niño3.4 Index (1890–1949)", fontsize=14, y=1.02)
fig.subplots_adjust(top=0.92)

plt.figtext(
    0.5, -0.02,
    "Figure: Standardized DJF Niño3.4 index (1890–1949) for three experiments.\n"
    "Orange shading: El Niño years (index ≥ +0.5σ); "
    "Purple shading: La Niña years (index ≤ –0.5σ).",
    ha="center", fontsize=11
)

plt.savefig("DJF_Nino34_1890-1949.png", dpi=300, bbox_inches="tight")
plt.show()



# -*- coding: utf-8 -*-
import xarray as xr
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
from pathlib import Path

# ===== dir=====
DATA_DIR = Path(r"D:\cangku\data")

GPP_FILES = {
    "abrupt-4xCO2": "gpp_Lmon_CanESM5_abrupt-4xCO2_r1i1p2f1_gn_185001-200012_1x1_1890-1949_detrend.nc",
    "G1":           "gpp_Lmon_CanESM5_G1_r1i1p2f1_gn_185001-194912_1x1_1890-1949_detrend.nc",
    "piControl":    "gpp_Lmon_CanESM5_piControl_r1i1p2f1_gn_185001-194912_1x1_1890-1949_detrend.nc",
}
AREA_FILE = DATA_DIR / "landarea_1x1.nc"   #  area(lat,lon)，单位 km²
LAT_SLICE = slice(-40, 40)

# ====== already ENSO_index, years ======
# ENSO_index: shape (3, 60)， DJF Niño3.4 index
# years:  1890–1949  60

def standardize_coords(ds):
    if "latitude" in ds.coords: ds = ds.rename({"latitude": "lat"})
    if "longitude" in ds.coords: ds = ds.rename({"longitude": "lon"})
    if "lon" in ds.coords and float(ds["lon"].min()) < 0:
        ds = ds.assign_coords(lon=(ds["lon"] % 360)).sortby("lon")
    return ds

def build_area_on_gpp_grid(area_path, gpp_ref_da):
    ds_area = xr.open_dataset(area_path)
    if "area" not in ds_area.data_vars:
        if len(ds_area.data_vars) == 1:
            only = list(ds_area.data_vars)[0]
            ds_area = ds_area.rename({only: "area"})
        else:
            raise KeyError("fsil find vars 'area'")
    ds_area = standardize_coords(ds_area)
    area = ds_area["area"].sel(lat=LAT_SLICE)
    area_matched = area.interp(lat=gpp_ref_da.lat, lon=gpp_ref_da.lon, method="nearest")
    return area_matched.broadcast_like(gpp_ref_da.isel(time=0))

def compute_annual_gpp_total(gpp_path, area_matched):
    ds = xr.open_dataset(gpp_path)
    ds = standardize_coords(ds)
    if "gpp" not in ds.data_vars:
        only = list(ds.data_vars)[0]
        ds = ds.rename({only: "gpp"})
    gpp = ds["gpp"].sel(lat=LAT_SLICE)

    area_use = area_matched.sel(lat=gpp.lat, lon=gpp.lon)
    area_use = xr.where(gpp.isel(time=0).isnull(), 0.0, area_use)

    days_in_month = gpp["time"].dt.days_in_month
    monthly_total = (gpp * area_use * days_in_month * 86400).sum(("lat","lon")) * 1e-12 * 1e6
    annual_total = monthly_total.resample(time="YS").sum().sel(time=slice("1890","1949"))
    return annual_total

# ===== main =====
gpp_ref = xr.open_dataset(DATA_DIR / GPP_FILES["piControl"])["gpp"]
area_ref = build_area_on_gpp_grid(AREA_FILE, gpp_ref)

GPP_totals = {}
for exp, fname in GPP_FILES.items():
    ann = compute_annual_gpp_total(DATA_DIR / fname, area_ref)
    GPP_totals[exp] = ann.values  # PgC/yr
years = ann["time.year"].values  # 共 60 年

# ===== draw picture =====
fig, axes = plt.subplots(3, 1, figsize=(10, 9), sharex=True)

for i, (exp, gpp_vals) in enumerate(GPP_totals.items()):
    ax = axes[i]
    enso_vals = ENSO_index[i, :]   # each ENSO index

    # background plot：ENSO index ≥ +0.5 或 ≤ –0.5
    ax.fill_between(years, -10, 10, where=enso_vals >= 0.5, color="orange", alpha=0.3)
    ax.fill_between(years, -10, 10, where=enso_vals <= -0.5, color="purple", alpha=0.3)

    # GPP year timeseries
    ax.plot(years, gpp_vals, color="black", linewidth=1.2)
    ax.scatter(years, gpp_vals, color="black", s=15, zorder=3)

    # y=0 
    ax.axhline(0, color="gray", linewidth=0.8, linestyle="--")

    ax.set_ylabel("GPP anomaly (PgC yr$^{-1}$)")
    ax.set_title(exp)

    # legend（ENSO）
    legend_handles = [
        Patch(facecolor="orange", alpha=0.3, label="El Niño (≥ +0.5σ)"),
        Patch(facecolor="purple", alpha=0.3, label="La Niña (≤ -0.5σ)")
    ]
    ax.legend(handles=legend_handles, loc="upper right", frameon=False)

axes[-1].set_xlabel("Year")
axes[-1].set_xlim(1890, 1950)
axes[-1].set_xticks(np.arange(1890, 1951, 5))

plt.tight_layout()

# save
fig.suptitle("Annual GPP Anomalies (40°S–40°N, 1890–1949)", fontsize=14, y=1.02)
fig.subplots_adjust(top=0.92)

plt.figtext(
    0.5, -0.02,
    "Figure: Annual GPP anomalies (40°S–40°N, 1890–1949) for three experiments.\n"
    "Orange shading: El Niño years (DJF Niño3.4 ≥ +0.5σ); "
    "Purple shading: La Niña years (DJF Niño3.4 ≤ –0.5σ).",
    ha="center", fontsize=11
)

plt.savefig("GPP_anomalies_40S40N_1890-1949.png", dpi=300, bbox_inches="tight")
plt.show()



import numpy as np
import matplotlib.pyplot as plt


# ENSO_index: shape (3, 60)  
# GPP_totals: dict -> turn into array (3, 60)
exp_names = list(GPP_totals.keys())  #  ['abrupt-4xCO2','G1','piControl']
GPP_array = np.vstack([GPP_totals[exp] for exp in exp_names])  # shape (3, 60)

# ===== regress =====
def regression_coef(x, y):
    mask = np.isfinite(x) & np.isfinite(y)
    if mask.sum() > 1:
        return np.cov(x[mask], y[mask], ddof=1)[0, 1] / np.var(x[mask], ddof=1)
    else:
        return np.nan

# ===== 回归计算 + bootstrap CI（单个试验） =====
betas, ci_lows, ci_highs = [], [], []
n_boot = 5000
rng = np.random.default_rng(seed=42)

for i in range(3):
    x = ENSO_index[i, :]
    y = GPP_array[i, :]
    beta = regression_coef(x, y)
    betas.append(beta)

    # bootstrap
    mask = np.isfinite(x) & np.isfinite(y)
    x_valid, y_valid = x[mask], y[mask]
    n = len(x_valid)
    boot_betas = []
    for _ in range(n_boot):
        idx = rng.integers(0, n, n)
        boot_betas.append(regression_coef(x_valid[idx], y_valid[idx]))
    ci_low, ci_high = np.percentile(boot_betas, [2.5, 97.5])
    ci_lows.append(ci_low)
    ci_highs.append(ci_high)

betas = np.array(betas)
ci_lows = np.array(ci_lows)
ci_highs = np.array(ci_highs)
yerr = np.vstack([betas - ci_lows, ci_highs - betas])

# ===== 试验之差（A−B）的 bootstrap CI =====
pairs = [("abrupt-4xCO2", "G1"),
         ("abrupt-4xCO2", "piControl"),
         ("G1", "piControl")]

pair_labels = [f"{a} – {b}" for a, b in pairs]  # 用 en-dash
diff_means, diff_err_low, diff_err_high, diff_sig = [], [], [], []

for expA, expB in pairs:
    iA, iB = exp_names.index(expA), exp_names.index(expB)
    xA, yA = ENSO_index[iA, :], GPP_array[iA, :]
    xB, yB = ENSO_index[iB, :], GPP_array[iB, :]

    maskA = np.isfinite(xA) & np.isfinite(yA)
    maskB = np.isfinite(xB) & np.isfinite(yB)
    xA, yA = xA[maskA], yA[maskA]
    xB, yB = xB[maskB], yB[maskB]

    nA, nB = len(xA), len(xB)
    boot_diffs = []
    for _ in range(n_boot):
        idxA = rng.integers(0, nA, nA)
        idxB = rng.integers(0, nB, nB)
        betaA = regression_coef(xA[idxA], yA[idxA])
        betaB = regression_coef(xB[idxB], yB[idxB])
        boot_diffs.append(betaA - betaB)

    boot_diffs = np.asarray(boot_diffs)
    mean_diff = np.nanmean(boot_diffs)
    ci_low, ci_high = np.percentile(boot_diffs, [2.5, 97.5])

    diff_means.append(mean_diff)
    diff_err_low.append(mean_diff - ci_low)
    diff_err_high.append(ci_high - mean_diff)
    diff_sig.append((ci_low > 0) or (ci_high < 0))  # CI 不跨 0 → 显著

diff_means = np.array(diff_means)
diff_yerr = np.vstack([diff_err_low, diff_err_high])

# ===== 画两排柱状图：上=三试验；下=三对差 =====
fig, axes = plt.subplots(2, 1, figsize=(7.2, 8), sharex=False)

# 顶部：三试验
ax = axes[0]
bars = ax.bar(
    exp_names, betas, yerr=yerr,
    color=["#FF7F0E", "#1F77B4", "#2CA02C"], alpha=0.85,
    capsize=5, error_kw=dict(lw=1.2)
)
for bar, beta in zip(bars, betas):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
            f"{beta:.2f}", ha="center", va="bottom", fontsize=10)
ax.axhline(0, color="gray", linewidth=0.8)
ax.set_ylabel("Regression coefficient (PgC yr$^{-1}$ σ$^{-1}$)")
ax.set_title("GPP on ENSO: Coefficients with 95% CI (1890–1949)")

# 底部：三对差
ax2 = axes[1]
bars2 = ax2.bar(
    pair_labels, diff_means, yerr=diff_yerr,
    color=["#FF7F0E", "#FF7F0E", "#1F77B4"], alpha=0.85,
    capsize=5, error_kw=dict(lw=1.2)
)
for i, bar in enumerate(bars2):
    y = bar.get_height()
    # 数值
    ax2.text(bar.get_x() + bar.get_width()/2, y,
             f"{y:.2f}", ha="center", va="bottom" if y>=0 else "top", fontsize=10)
    # 显著性星号（CI 不跨 0）
    if diff_sig[i]:
        ax2.text(bar.get_x() + bar.get_width()/2, y + (0.02 if y>=0 else -0.02),
                 "*", ha="center", va="bottom" if y>=0 else "top", fontsize=16, color="black")

ax2.axhline(0, color="gray", linewidth=0.8)
ax2.set_ylabel("Difference in coefficient (PgC yr$^{-1}$ σ$^{-1}$)")
ax2.set_title("Pairwise Differences (A – B) with 95% CI")

plt.subplots_adjust(left=0.15, right=0.98, top=0.90, bottom=0.18, hspace=0.35)

# 图注
plt.figtext(
    0.5, 0.02,
    "Bars show regression of annual GPP anomalies (40°S–40°N) on standardized DJF Niño3.4.\n"
    "Error bars: 95% bootstrap CI (n=5000). Asterisk (*) indicates the difference CI does not cross zero.",
    ha="center", fontsize=9
)

plt.savefig("GPP_ENSO_regression_coeffs_and_differences.png", dpi=300, bbox_inches="tight")
plt.show()



# --- A. 生成逐格点逐年 GPP (time=60, lat, lon) ---
import xarray as xr
import numpy as np
from pathlib import Path

DATA_DIR = Path(r"D:\cangku\data")
GPP_FILES = {
    "abrupt-4xCO2": "gpp_Lmon_CanESM5_abrupt-4xCO2_r1i1p2f1_gn_185001-200012_1x1_1890-1949_detrend.nc",
    "G1":           "gpp_Lmon_CanESM5_G1_r1i1p2f1_gn_185001-194912_1x1_1890-1949_detrend.nc",
    "piControl":    "gpp_Lmon_CanESM5_piControl_r1i1p2f1_gn_185001-194912_1x1_1890-1949_detrend.nc",
}
LAT_SLICE = slice(-40, 40)

def _std_coords(ds):
    if "latitude" in ds.coords:  ds = ds.rename({"latitude":"lat"})
    if "longitude" in ds.coords: ds = ds.rename({"longitude":"lon"})
    if "lon" in ds.coords and float(ds["lon"].min()) < 0:
        ds = ds.assign_coords(lon=(ds["lon"] % 360)).sortby("lon")
    return ds

def compute_annual_gpp_field(gpp_path: Path):
    """月度 gpp(kgC m^-2 s^-1) → 年累计 gpp(kgC m^-2 yr^-1)，1890–1949"""
    ds = xr.open_dataset(gpp_path)
    ds = _std_coords(ds)
    var = "gpp" if "gpp" in ds.data_vars else list(ds.data_vars)[0]
    gpp = ds[var].sel(lat=LAT_SLICE)  # (time, lat, lon), kgC m^-2 s^-1（已线性去趋势）
    days = gpp["time"].dt.days_in_month
    # integrate month → year
    gpp_annual = (gpp * days * 86400).resample(time="YS").sum().sel(time=slice("1890","1949"))
    return gpp_annual.astype("float32")  # (60, lat, lon)

# 构建字典：每个试验的逐格点逐年 GPP
GPP_annual = {}
for exp, fname in GPP_FILES.items():
    da = compute_annual_gpp_field(DATA_DIR / fname)
    print(f"{exp}: shape={da.shape}, years={int(da.time.dt.year.min())}-{int(da.time.dt.year.max())}")
    GPP_annual[exp] = da

# 检查与 ENSO_index 对齐（应为 60 年）
print("ENSO_index shape:", ENSO_index.shape)  # 期待 (3, 60)



import numpy as np
import matplotlib.pyplot as plt
import cartopy.crs as ccrs

# ===== 参数 =====
exp_order = ["abrupt-4xCO2", "G1", "piControl"]

def regression_coef(x, y):
    mask = np.isfinite(x) & np.isfinite(y)
    if mask.sum() > 1:
        return np.cov(x[mask], y[mask], ddof=1)[0, 1] / np.var(x[mask], ddof=1)
    return np.nan

def grid_regression_quick(gpp_annual_da, enso_series):
    gpp = gpp_annual_da.values
    t, nlat, nlon = gpp.shape
    betas = np.full((nlat, nlon), np.nan, dtype="float32")
    for i in range(nlat):
        for j in range(nlon):
            y = gpp[:, i, j]
            if np.all(np.isnan(y)): 
                continue
            betas[i, j] = regression_coef(enso_series, y)
    return betas

# ===== 1) 三个试验各自回归 =====
beta_maps = {}
for i, exp in enumerate(exp_order):
    betas = grid_regression_quick(GPP_annual[exp], ENSO_index[i, :])
    beta_maps[exp] = betas
    print(f"[{exp}] beta: min={np.nanmin(betas):.3f}, max={np.nanmax(betas):.3f}")

# ===== 2) 三个差值 =====
diff_pairs = [("abrupt-4xCO2","G1"),
              ("abrupt-4xCO2","piControl"),
              ("G1","piControl")]
diff_maps = {f"{A}-{B}": beta_maps[A] - beta_maps[B] for (A,B) in diff_pairs}

# ===== 3) 色卡范围 =====
vmin, vmax = -0.05, 0.05
cbar_ticks = np.arange(-0.05, 0.051, 0.01)

# ===== 4) 绘图 =====
fig, axes = plt.subplots(2, 3, figsize=(16, 7),
                         subplot_kw={"projection": ccrs.PlateCarree()})

titles = [
    r"abrupt-4xCO2: $\gamma_{GPP}^{ENSO}$",
    r"G1: $\gamma_{GPP}^{ENSO}$",
    r"piControl: $\gamma_{GPP}^{ENSO}$",
    r"abrupt-4xCO2 − G1",
    r"abrupt-4xCO2 − piControl",
    r"G1 − piControl"
]

def plot_panel(ax, field, lats, lons, title, vmin, vmax):
    im = ax.pcolormesh(lons, lats, field, cmap="RdBu_r",
                       vmin=vmin, vmax=vmax, transform=ccrs.PlateCarree())
    ax.coastlines(linewidth=0.6)
    # 加经纬度网格
    gl = ax.gridlines(draw_labels=True, linewidth=0.3, color='gray', alpha=0.5, linestyle='--')
    gl.top_labels = False
    gl.right_labels = False
    gl.xlabel_style = {"size":8}
    gl.ylabel_style = {"size":8}
    ax.set_title(title, fontsize=11)
    return im

# 上排：三个试验
for col, exp in enumerate(exp_order):
    im = plot_panel(axes[0, col], beta_maps[exp],
                    GPP_annual[exp].lat.values, GPP_annual[exp].lon.values,
                    titles[col], vmin, vmax)

# 下排：三个差值
for col, key in enumerate([f"{a}-{b}" for (a,b) in diff_pairs]):
    im = plot_panel(axes[1, col], diff_maps[key],
                    GPP_annual[exp_order[0]].lat.values, GPP_annual[exp_order[0]].lon.values,
                    titles[3+col], vmin, vmax)

# 共享色标
cbar_ax = fig.add_axes([0.25, 0.17, 0.5, 0.02])
cb = fig.colorbar(im, cax=cbar_ax, orientation="horizontal", ticks=cbar_ticks)
cb.set_label(r"$\gamma_{GPP}^{ENSO}$ (kgC m$^{-2}$ yr$^{-1}$ σ$^{-1}$)")

# 图整体标题 + 说明
fig.suptitle("GPP–ENSO Regression Coefficients (40°S–40°N, 1890–1949)", fontsize=14, y=0.96)
plt.figtext(0.5, 0.01,
    r"Figure: Spatial distribution of $\gamma_{GPP}^{ENSO}$ for three experiments (top row) "
    "and their pairwise differences (bottom row).\n"
    "Color scale fixed to [-0.05, 0.05] with 0.01 intervals.",
    ha="center", fontsize=10)

# 调整布局：更紧凑
plt.subplots_adjust(left=0.06, right=0.96, top=0.90, bottom=0.22,
                    wspace=0.12, hspace=0.15)

plt.savefig("GPP_ENSO_regression_maps_final.png", dpi=300)
plt.show()



import matplotlib.pyplot as plt
import cartopy.crs as ccrs
from matplotlib.patches import Rectangle

# 区域定义（0–360 经度体系）
regions_demo = {
    "Amazon":     {"lat": (-30, 10),  "lon": (275, 325), "color": "green"},
    "Africa":     {"lat": (-10, 10), "lon": (10, 45),   "color": "red"},
    "S/SE Asia":  {"lat": (-10, 25), "lon": (70, 160),  "color": "blue"},
    "Australia":  {"lat": (-40, -10),  "lon": (110, 160), "color": "orange"},
}

fig = plt.figure(figsize=(9, 5))
ax = plt.axes(projection=ccrs.PlateCarree())

# 设置显示范围，避免 0=360 singular 问题
ax.set_extent([1e-3, 359.999, -40, 40], crs=ccrs.PlateCarree())
ax.coastlines()

# 绘制区域矩形
for name, b in regions_demo.items():
    lat1, lat2 = b["lat"]
    lon1, lon2 = b["lon"]
    rect = Rectangle(
        (lon1, lat1), lon2 - lon1, lat2 - lat1,
        linewidth=1, edgecolor="k", facecolor=b["color"], alpha=0.4,
        transform=ccrs.PlateCarree()
    )
    ax.add_patch(rect)
    ax.text((lon1+lon2)/2, (lat1+lat2)/2, name,
            ha="center", va="center", fontsize=10, weight="bold",
            transform=ccrs.PlateCarree(),
            bbox=dict(facecolor="white", alpha=0.6, edgecolor="none"))

plt.title("Analysis Regions (40°S–40°N)", fontsize=13)
plt.savefig("GPP_ENSO_regions_demo.png", dpi=300, bbox_inches="tight")
plt.show()



import xarray as xr
import numpy as np
from matplotlib.patches import Rectangle

# === 统一坐标工具 ===
def std_coords_da(da):
    """把 DataArray 的坐标/维度统一成 lat/lon，并把经度转 0..360、按 lon 排序"""
    if "latitude" in da.dims or "longitude" in da.dims:
        rename = {}
        if "latitude" in da.dims:  rename["latitude"]  = "lat"
        if "longitude" in da.dims: rename["longitude"] = "lon"
        da = da.rename(rename)
    if "lat" not in da.dims or "lon" not in da.dims:
        raise ValueError(f"unexpected dims: {da.dims}")
    # 经度到 0..360
    if float(da["lon"].min()) < 0:
        da = da.assign_coords(lon=(da["lon"] % 360)).sortby("lon")
    return da

# === 读取面积并对齐到 GPP 网格（注意：km² → m²）===
AREA_FILE = DATA_DIR / "landarea_1x1.nc"
_area_ds = xr.open_dataset(AREA_FILE)
area_var = "area" if "area" in _area_ds.data_vars else list(_area_ds.data_vars)[0]
area_da  = std_coords_da(_area_ds[area_var])              # 维度修复到 (lat, lon)
area_da  = area_da.interp(lat=GPP_annual["piControl"].lat,
                          lon=GPP_annual["piControl"].lon,
                          method="nearest")
area_m2  = (area_da * 1e6).astype("float32")              # km² → m²

# === 区域定义（经度用 0..360 表示西经）===
regions = {
    "Amazon":     {"lat": (-15, 5),  "lon": (285, 310)},  # -75°–-50°W
    "Africa":     {"lat": (-10, 10), "lon": (10, 40)},
    "SE Asia":    {"lat": (-10, 20), "lon": (100, 120)},
    "Australia":  {"lat": (-20, 0),  "lon": (120, 150)},
    "South Asia": {"lat": (5, 25),   "lon": (70, 95)},
}

# === 在地图上叠加矩形框（带标签） ===
def add_region_boxes(ax, regions, label=True):
    for name, b in regions.items():
        lat1, lat2 = b["lat"]
        lon1, lon2 = b["lon"]
        rect = Rectangle(
            (lon1, lat1), lon2 - lon1, lat2 - lat1,
            linewidth=1.2, edgecolor="k", facecolor="none", zorder=5,
            transform=ccrs.PlateCarree()
        )
        ax.add_patch(rect)
        if label:
            ax.text((lon1+lon2)/2, (lat1+lat2)/2, name,
                    transform=ccrs.PlateCarree(),
                    ha="center", va="center", fontsize=8,
                    bbox=dict(facecolor="white", alpha=0.6, edgecolor="none"),
                    zorder=6)



# -*- coding: utf-8 -*-
import numpy as np
import matplotlib.pyplot as plt
import xarray as xr

# ===== 区域定义（与你给的一致，0–360 经度体系）=====
regions_demo = {
    "Amazon":     {"lat": (-30, 10),  "lon": (275, 325), "color": "green"},
    "Africa":     {"lat": (-10, 10),  "lon": (10, 45),   "color": "red"},
    "S/SE Asia":  {"lat": (-10, 25),  "lon": (70, 160),  "color": "blue"},
    "Australia":  {"lat": (-40, -10), "lon": (110, 160), "color": "orange"},
}

# ===== 实验与差值配对（与空间图一致）=====
exp_order  = ["abrupt-4xCO2", "G1", "piControl"]
diff_pairs = [("abrupt-4xCO2","G1"),
              ("abrupt-4xCO2","piControl"),
              ("G1","piControl")]

# ===== 将 numpy 的 beta_maps 包装为 DataArray，方便区域切片 =====
# 要求：area_m2 是 xr.DataArray，且 lat/lon 与 beta_maps 网格一致
beta_da = {}
for exp in exp_order:
    betas = beta_maps[exp]  # (lat, lon), 单位 kgC m^-2 yr^-1 σ^-1
    beta_da[exp] = xr.DataArray(
        betas, coords={"lat": area_m2.lat, "lon": area_m2.lon}, dims=("lat","lon")
    )

# ===== 计算“区域总敏感性”（PgC yr^-1 σ^-1）=====
def region_total_sensitivity(beta_field: xr.DataArray, area_m2: xr.DataArray, reg_bounds: dict):
    """在给定矩形区域内，sum( beta * area )，并做 kg→Pg 的换算"""
    lat1, lat2 = reg_bounds["lat"]
    lon1, lon2 = reg_bounds["lon"]
    sub_beta = beta_field.sel(lat=slice(lat1, lat2), lon=slice(lon1, lon2))
    sub_area = area_m2.sel( lat=slice(lat1, lat2), lon=slice(lon1, lon2))
    total_kg_per_yr_sigma = (sub_beta * sub_area).sum().item()  # kg yr^-1 σ^-1
    total_Pg_per_yr_sigma = total_kg_per_yr_sigma * 1e-12       # Pg yr^-1 σ^-1
    return total_Pg_per_yr_sigma

# 顶排：三个试验的区域总敏感性
region_totals = {exp: {} for exp in exp_order}
for exp in exp_order:
    for reg_name, bounds in regions_demo.items():
        region_totals[exp][reg_name] = region_total_sensitivity(beta_da[exp], area_m2, bounds)

# 底排：三对试验差值（A−B）的区域总敏感性差
region_diffs = {}
for A, B in diff_pairs:
    key = f"{A} − {B}"
    region_diffs[key] = {}
    for reg_name in regions_demo.keys():
        region_diffs[key][reg_name] = region_totals[A][reg_name] - region_totals[B][reg_name]

# ===== 统一 y 轴范围（对所有面板取对称范围）=====
all_vals = []
for exp in exp_order:
    all_vals.extend(region_totals[exp].values())
for key in region_diffs:
    all_vals.extend(region_diffs[key].values())
ymax = max(1e-9, np.nanmax(np.abs(all_vals)))  # 避免全 0 导致异常
ymax = float(ymax * 1.10)                      # 10% 余量
ylims = (-ymax, ymax)

# ===== 画 2×3 子图 =====
fig, axes = plt.subplots(2, 3, figsize=(14, 6), sharey=True)

def plot_region_bars(ax, values_dict, title):
    regs   = list(regions_demo.keys())
    vals   = [values_dict[r] for r in regs]
    colors = [regions_demo[r]["color"] for r in regs]
    x = np.arange(len(regs))
    bars = ax.bar(x, vals, color=colors, alpha=0.85)
    ax.axhline(0, color="gray", lw=0.8)
    ax.set_xticks(x)
    ax.set_xticklabels(regs, rotation=20)
    ax.set_ylim(*ylims)
    ax.set_title(title, fontsize=11)
    # 数值标签
    for b, v in zip(bars, vals):
        ax.text(b.get_x()+b.get_width()/2, v,
                f"{v:.2f}", ha="center", va="bottom" if v>=0 else "top", fontsize=9)

# 上排：三个试验
for col, exp in enumerate(exp_order):
    plot_region_bars(axes[0, col], region_totals[exp],
                     title=fr"{exp}: regional $\sum\ \gamma_{{GPP}}^{{ENSO}}$")

# 下排：三个差值
for col, (A, B) in enumerate(diff_pairs):
    key = f"{A} − {B}"
    plot_region_bars(axes[1, col], region_diffs[key],
                     title=fr"{A} − {B}: regional $\Delta\sum\ \gamma_{{GPP}}^{{ENSO}}$")

# 统一标签与标题
axes[0,0].set_ylabel("Sensitivity (PgC yr$^{-1}$ $\sigma^{-1}$)")
axes[1,0].set_ylabel("Sensitivity (PgC yr$^{-1}$ $\sigma^{-1}$)")
fig.suptitle("Regional GPP–ENSO Sensitivity (40°S–40°N, 1890–1949)", fontsize=14, y=0.99)

# 更紧凑布局
plt.subplots_adjust(left=0.07, right=0.98, top=0.92, bottom=0.10, wspace=0.20, hspace=0.35)

# 图注
plt.figtext(0.5, 0.00,
    "Bars show area-weighted totals of grid-level sensitivity (kgC m$^{-2}$ yr$^{-1}$ $\\sigma^{-1}$) "
    r"integrated over each region and converted to PgC yr$^{-1}$ $\sigma^{-1}$. "
    "Colors match the region map: Amazon (green), Africa (red), S/SE Asia (blue), Australia (orange).",
    ha="center", fontsize=9)

plt.savefig("GPP_ENSO_regional_sensitivity_6panels.png", dpi=300)
plt.show()




